{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q http://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES FOR PROJECT\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM FUNCTIONS\n",
    "import random\n",
    "\n",
    "def datasetBalancing(originalDict):\n",
    "    \"\"\"\n",
    "    This function has the purpose of balancing the dataset before using it in CNN.\n",
    "    \n",
    "    Starting from input data, we separate two distinct classes: one of the original classes\n",
    "    extracted from the dataset that will act as a leaf (we call it 'class0'), and, excluding \n",
    "    the last step (consisting of only the two remaining classes) one \"super\" class containing \n",
    "    all the elements remaining (we call it 'class1')\n",
    "    \n",
    "    To keep the dataset balanced, we need the size of class0 to be equal to the size of class1. \n",
    "    In order to do that, we compute (based both on the size of class0 and the number of the \n",
    "    different classes in class1) the number of elements that have to be extracted from each\n",
    "    subclass of class1 and we take them after reshuffling the elements to avoid taking\n",
    "    always the same.\n",
    "    \n",
    "    :param (dictionary) originalDict: input dataset\n",
    "    \n",
    "    :return (dictionary) reducedDict: the input dataset with one class dropped\n",
    "    :return (array) values: the values extracted from dataset\n",
    "    :return (array) labels: the two key, one of one class and the key 'others' for the super class\n",
    "    \"\"\"\n",
    "    \n",
    "    classToDrop = list(originalDict)[len(originalDict.keys())-1]\n",
    "    \n",
    "    class0, class1 = dropClass(originalDict, classToDrop)\n",
    "\n",
    "    print('Class ' + classToDrop + ' dropped')\n",
    "\n",
    "    if len(class1.keys()) == 1:\n",
    "        #TODO: SET A TRESHOLD TO DETERMINE IF ALSO WITH LAST 2 CLASSES WE DON'T KNOW WHICH ONE IS FOR SURE\n",
    "        dictForCNN = {list(class1.keys())[0]: class1[list(class1.keys())[0]], classToDrop: class0}\n",
    "        values, labels = returnValues(dictForCNN)\n",
    "        return class1, values, labels\n",
    "\n",
    "    elementsFromEachClass = int(round(len(class0) / len(class1.keys())))\n",
    "    \n",
    "    print('We will take ' + str(elementsFromEachClass) + ' elements from each class ')\n",
    "    \n",
    "    class1_balanced = []\n",
    "    for k in class1.keys():\n",
    "        myClassElements = class1[k]\n",
    "        random.shuffle(myClassElements)\n",
    "        class1_balanced += myClassElements[:elementsFromEachClass]\n",
    "    \n",
    "    dictForCNN = {}\n",
    "    \n",
    "    dictForCNN.update({'others': []})\n",
    "    dictForCNN.update({classToDrop: []})\n",
    "        \n",
    "    for i in class1_balanced:\n",
    "        dictForCNN['others'].append(i)\n",
    "    for i in class0:\n",
    "        dictForCNN[classToDrop].append(i)\n",
    "        \n",
    "    values, labels = returnValues(dictForCNN)\n",
    "        \n",
    "    return class1, values, labels\n",
    "\n",
    "def dropClass(dictInput, className):\n",
    "    \"\"\"\n",
    "    Function with the aim of dropping one class from a dictionary\n",
    "    \n",
    "    :param (dictionary) dictInput: the dictionary\n",
    "    :param (string) className: the name of the class that has to be dropped from dictInput\n",
    "    \n",
    "    :return (array) class0: the elements contained in the dropped class\n",
    "    :return (dictionary) class1: the remainig dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    return dictInput.pop(className), dictInput\n",
    "\n",
    "def toDict(trainset):\n",
    "    \"\"\"\n",
    "    Function with the goal of taking labels and data putting them in a dictionary.\n",
    "    We take the data with its label and insert it into a dictionary: in the end, \n",
    "    we will have a dictionary with the different classes as keys and all the \n",
    "    data with that label as values.\n",
    "    \n",
    "    :param (dataset) trainset: the trainset\n",
    "    \n",
    "    :return (dictionary) returnDict: the resultant dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    returnDict = {}\n",
    "    \n",
    "    for data, label in zip(trainset.data, trainset.targets):\n",
    "        label = str(label)\n",
    "        if label in returnDict.keys():\n",
    "            returnDict[label].append(data)\n",
    "        else:\n",
    "            returnDict.update({label: []})\n",
    "            returnDict[label].append(data)\n",
    "            \n",
    "    return returnDict\n",
    "\n",
    "\"\"\"\n",
    "PROBABILMENTE NON SERVE A UN CAZZO\n",
    "\n",
    "def manageTestset(testset, className):\n",
    "    returnDict = {'others': [], className: []}\n",
    "    \n",
    "    for data, label in zip(testset.data, testset.targets):\n",
    "        label = str(label)\n",
    "        if label != className:\n",
    "            returnDict['others'].append(data)\n",
    "        else:\n",
    "            returnDict[className].append(data)\n",
    "\"\"\"\n",
    "            \n",
    "def returnValues(dictInput):\n",
    "    \"\"\"\n",
    "    Simply function that takes in input a dictionary and returns two array of values and labels before \n",
    "    passing them into the DataLoader\n",
    "    \n",
    "    :param (dictionary) dictInput: the input dictionary\n",
    "    \n",
    "    :return (array) values: all the values\n",
    "    :return (array) labels: all the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    values = np.array(list(dictInput.values())[0] + list(dictInput.values())[1])\n",
    "    labels = [1]*len(list(dictInput.values())[0]) + [0]*len(list(dictInput.values())[1])\n",
    "    \n",
    "    return values, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET CLEANING\n",
    "\n",
    "Since the patient_id is not useful for classification purpose,\n",
    "we move the dataset into a folder containing X sub-folders\n",
    "where X is defined as the number of possible classes.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "def refactor_dataset():    \n",
    "    for count, folder_name in enumerate(os.listdir(\"./DatasetINPUT/\")):\n",
    "        for file_name in os.listdir(\"./DatasetINPUT/\" + folder_name):\n",
    "            os.rename(file_name, \"./DatasetOUTPUT/\" + file_name.split(\"_\")[1] + \"/\" + file_name)\n",
    "    os.remove(\"DatasetINPUT\")\n",
    "    os.rename(\"DatasestOUTPUT\", \"Dataset\")\n",
    "            \n",
    "def make_folders_tree(folder_names):\n",
    "    for folder_name in folder_names:\n",
    "        mkdir_if_not_exist(folder_name)\n",
    "               \n",
    "@staticmethod\n",
    "def mkdir_if_not_exist(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "make_folders_tree([\"AC\", \"H\", \"Serr\", \"T\", \"V\"])\n",
    "refactor_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_kernel(model):\n",
    "    model_weights = model.state_dict()\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for idx, filt  in enumerate(model_weights['conv1.weight']):\n",
    "    #print(filt[0, :, :])\n",
    "        if idx >= 32: continue\n",
    "        plt.subplot(4,8, idx + 1)\n",
    "        plt.imshow(filt[0, :, :], cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_kernel_output(model,images):\n",
    "    fig1 = plt.figure()\n",
    "    plt.figure(figsize=(1,1))\n",
    "    \n",
    "    img_normalized = (images[0] - images[0].min()) / (images[0].max() - images[0].min())\n",
    "    plt.imshow(img_normalized.numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    output = model.conv1(images)\n",
    "    layer_1 = output[0, :, :, :]\n",
    "    layer_1 = layer_1.data\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for idx, filt  in enumerate(layer_1):\n",
    "        if idx >= 32: continue\n",
    "        plt.subplot(4,8, idx + 1)\n",
    "        plt.imshow(filt, cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def test_accuracy(net, dataloader):\n",
    "  ########TESTING PHASE###########\n",
    "  \n",
    "    #check accuracy on whole test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval() #important for deactivating dropout and correctly use batchnorm accumulated statistics\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy of the network on the test set: %d %%' % (accuracy))\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "n_classes = 2       \n",
    "      \n",
    "#function to define the convolutional network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #conv2d first parameter is the number of kernels at input (you get it from the output value of the previous layer)\n",
    "        #conv2d second parameter is the number of kernels you wanna have in your convolution, so it will be the n. of kernels at output.\n",
    "        #conv2d third, fourth and fifth parameters are, as you can read, kernel_size, stride and zero padding :)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv_final = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 4096)\n",
    "        self.fc2 = nn.Linear(4096, n_classes) #last FC for classification \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.pool(self.conv_final(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #hint: dropout goes here!\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "      ####RUNNING CODE FROM HERE:\n",
    "      \n",
    "#transform are heavily used to do simple and complex transformation and data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "     #transforms.RandomHorizontalFlip(),\n",
    "     transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)\n",
    "\n",
    "#mydict = toDict(trainset)\n",
    "trainset_old, trainset.data, trainset.targets = datasetBalancing(toDict(trainset))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                          shuffle=True, num_workers=4,drop_last=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=4,drop_last=True)\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "#show images just to understand what is inside the dataset\n",
    "#images, labels = dataiter.next()\n",
    "#imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "###OPTIONAL:\n",
    "#print(\"####plotting kernels of conv1 layer:####\")\n",
    "#plot_kernel(net)\n",
    "####\n",
    "\n",
    "\n",
    "net = net.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() #it already does softmax computation for use!\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001) #better convergency w.r.t simple SGD :)\n",
    "\n",
    "###OPTIONAL:\n",
    "#print(\"####plotting output of conv1 layer:#####\")\n",
    "#plot_kernel_output(net,images)  \n",
    "###\n",
    "\n",
    "########TRAINING PHASE###########\n",
    "n_loss_print = len(trainloader)  #print every epoch, use smaller numbers if you wanna print loss more often!\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    net.train() #important for activating dropout and correctly train batchnorm\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs and cast them into cuda wrapper\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % n_loss_print == (n_loss_print -1):    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / n_loss_print))\n",
    "            running_loss = 0.0\n",
    "    test_accuracy(net,testloader)\n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
