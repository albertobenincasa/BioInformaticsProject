{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: torch-1.1.0-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\n",
      "WARNING: You are using pip version 19.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\alberto\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\alberto\\anaconda3\\lib\\site-packages (from torchvision) (5.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alberto\\anaconda3\\lib\\site-packages (from torchvision) (1.16.0)\n",
      "Requirement already satisfied: six in c:\\users\\alberto\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: torch in c:\\users\\alberto\\anaconda3\\lib\\site-packages (from torchvision) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q http://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES FOR PROJECT\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM FUNCTIONS\n",
    "\n",
    "def datasetBalancing(originalDict, nClassesToDrop):\n",
    "    \"\"\"\n",
    "    This function has the purpose of balancing the dataset before using it in CNN.\n",
    "    \n",
    "    Starting from input data, we separate two distinct classes: one of the original classes\n",
    "    extracted from the dataset that will act as a leaf (we call it 'class0'), and, excluding \n",
    "    the last step (consisting of only the two remaining classes) one \"super\" class containing \n",
    "    all the elements remaining (we call it 'class1')\n",
    "    \n",
    "    To keep the dataset balanced, we need the size of class0 to be equal to the size of class1. \n",
    "    In order to do that, we compute (based both on the size of class0 and the number of the \n",
    "    different classes in class1) the number of elements that have to be extracted from each\n",
    "    subclass of class1 and we take them after reshuffling the elements to avoid taking\n",
    "    always the same.\n",
    "    \n",
    "    :param (dictionary) originalDict: input dataset\n",
    "    \n",
    "    :return (dictionary) reducedDict: the input dataset with one class dropped\n",
    "    :return (array) values: the values extracted from dataset\n",
    "    :return (array) labels: the two key, one of one class and the key 'others' for the super class\n",
    "    \"\"\"\n",
    "    tempDict = originalDict.copy()\n",
    "    \n",
    "    print(\"Start rebalancing: classes to drop = \" + str(nClassesToDrop))\n",
    "    for index in range(0, nClassesToDrop + 1):\n",
    "      classToDrop = list(tempDict)[0]\n",
    "      class0, tempDict = dropClass(tempDict, classToDrop)\n",
    "      if len(tempDict.keys()) == 1:\n",
    "        print(\"Last step: only 2 classes remaining\")\n",
    "        #TODO: SET A TRESHOLD TO DETERMINE IF ALSO WITH LAST 2 CLASSES WE DON'T KNOW WHICH ONE IS FOR SURE\n",
    "        dictForCNN = {list(tempDict.keys())[0]: tempDict[list(tempDict.keys())[0]], classToDrop: class0}\n",
    "        values, labels = returnValues(dictForCNN)\n",
    "        return values, labels\n",
    "      print('Class ' + str(classToDrop) + ' dropped')\n",
    "      \n",
    "    class1 = tempDict.copy()\n",
    "    #class0, class1 = dropClass(originalDict, classToDrop)\n",
    "\n",
    "    elementsFromEachClass = int(round(len(class0) / len(class1.keys())))\n",
    "    \n",
    "    print('We will take ' + str(elementsFromEachClass) + ' elements from each class ')\n",
    "    \n",
    "    class1_balanced = []\n",
    "    for k in class1.keys():\n",
    "        myClassElements = class1[k]\n",
    "        random.shuffle(myClassElements)\n",
    "        class1_balanced += myClassElements[:elementsFromEachClass]\n",
    "    \n",
    "    dictForCNN = {}\n",
    "    \n",
    "    dictForCNN.update({'others': []})\n",
    "    dictForCNN.update({classToDrop: []})\n",
    "        \n",
    "    for i in class1_balanced:\n",
    "        dictForCNN['others'].append(i)\n",
    "    for i in class0:\n",
    "        dictForCNN[classToDrop].append(i)\n",
    "        \n",
    "    values, labels = returnValues(dictForCNN)\n",
    "        \n",
    "    return values, labels\n",
    "\n",
    "def dropClass(dictInput, className):\n",
    "    \"\"\"\n",
    "    Function with the aim of dropping one class from a dictionary\n",
    "    \n",
    "    :param (dictionary) dictInput: the dictionary\n",
    "    :param (string) className: the name of the class that has to be dropped from dictInput\n",
    "    \n",
    "    :return (array) class0: the elements contained in the dropped class\n",
    "    :return (dictionary) class1: the remainig dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(dictInput)\n",
    "    #print(className)\n",
    "    #classNameString=str(className)\n",
    "    return dictInput.pop(className), dictInput\n",
    "    #return dictInput.remove(classNameString), dictInput\n",
    "    \n",
    "def toDict(trainset):\n",
    "    \"\"\"\n",
    "    Function with the goal of taking labels and data putting them in a dictionary.\n",
    "    We take the data with its label and insert it into a dictionary: in the end, \n",
    "    we will have a dictionary with the different classes as keys and all the \n",
    "    data with that label as values.\n",
    "    \n",
    "    :param (dataset) trainset: the trainset\n",
    "    \n",
    "    :return (dictionary) returnDict: the resultant dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    returnDict = {}\n",
    "    \n",
    "    for data, label in zip(trainset.data, trainset.targets):\n",
    "        label = str(label)\n",
    "        if label in returnDict.keys():\n",
    "            returnDict[label].append(data)\n",
    "        else:\n",
    "            returnDict.update({label: []})\n",
    "            returnDict[label].append(data)\n",
    "            \n",
    "    return returnDict\n",
    "\n",
    "\"\"\"\n",
    "PROBABILMENTE NON SERVE A UN CAZZO\n",
    "\n",
    "def manageTestset(testset, className):\n",
    "    returnDict = {'others': [], className: []}\n",
    "    \n",
    "    for data, label in zip(testset.data, testset.targets):\n",
    "        label = str(label)\n",
    "        if label != className:\n",
    "            returnDict['others'].append(data)\n",
    "        else:\n",
    "            returnDict[className].append(data)\n",
    "\"\"\"\n",
    "            \n",
    "def returnValues(dictInput):\n",
    "    \"\"\"\n",
    "    Simply function that takes in input a dictionary and returns two array of values and labels before \n",
    "    passing them into the DataLoader\n",
    "    \n",
    "    :param (dictionary) dictInput: the input dictionary\n",
    "    \n",
    "    :return (array) values: all the values\n",
    "    :return (array) labels: all the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    values = np.array(list(dictInput.values())[0] + list(dictInput.values())[1])\n",
    "    labels = [1]*len(list(dictInput.values())[0]) + [0]*len(list(dictInput.values())[1])\n",
    "    \n",
    "    return values, labels\n",
    "\n",
    "def test_accuracy(tree, dataloader, originalDict):\n",
    "  ########TESTING PHASE###########\n",
    "  \n",
    "    #check accuracy on whole test set\n",
    "    TotalClass = len(originalDict)\n",
    "    leaves=originalDict\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval() #important for deactivating dropout and correctly use batchnorm accumulated statistics\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            \n",
    "                      \n",
    "            for i in range(0,TotalClass-1):\n",
    "              \n",
    "                labels_renamed=labels #cosÃ¬ non perdo l'informazione\n",
    "                if labels_renamed == leaves[i]:\n",
    "                    labels_renamed = torch.tensor(1)\n",
    "\n",
    "                else:\n",
    "                    labels_renamed = torch.tensor(0)\n",
    "                    \n",
    "\n",
    "                labels_renamed = labels_renamed.cuda()  \n",
    "                outputs =tree[i](images)  \n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                if predicted == 1:\n",
    "                    class_predicted=leaves[i]\n",
    "                    break\n",
    "                  \n",
    "                class_predicted=leaves[i+1]\n",
    "\n",
    "            if class_predicted == labels:\n",
    "                correct=correct + 1\n",
    "                total += labels.size(0)\n",
    "                #correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy of the network on the test set: %d %%' % (accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET CLEANING\n",
    "\n",
    "Since the patient_id is not useful for classification purpose,\n",
    "we move the dataset into a folder containing X sub-folders\n",
    "where X is defined as the number of possible classes.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "def refactor_dataset():    \n",
    "    for count, folder_name in enumerate(os.listdir(\"./DatasetINPUT/\")):\n",
    "        for file_name in os.listdir(\"./DatasetINPUT/\" + folder_name):\n",
    "            os.rename(file_name, \"./DatasetOUTPUT/\" + file_name.split(\"_\")[1] + \"/\" + file_name)\n",
    "    os.remove(\"DatasetINPUT\")\n",
    "    os.rename(\"DatasestOUTPUT\", \"Dataset\")\n",
    "            \n",
    "def make_folders_tree(folder_names):\n",
    "    for folder_name in folder_names:\n",
    "        mkdir_if_not_exist(folder_name)\n",
    "               \n",
    "@staticmethod\n",
    "def mkdir_if_not_exist(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "make_folders_tree([\"AC\", \"H\", \"Serr\", \"T\", \"V\"])\n",
    "refactor_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dd54ecf00d17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[0mtrainset_gold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m \u001b[0mtotalClasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#number of classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'targets'"
     ]
    }
   ],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_kernel(model):\n",
    "    model_weights = model.state_dict()\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for idx, filt  in enumerate(model_weights['conv1.weight']):\n",
    "    #print(filt[0, :, :])\n",
    "        if idx >= 32: continue\n",
    "        plt.subplot(4,8, idx + 1)\n",
    "        plt.imshow(filt[0, :, :], cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_kernel_output(model,images):\n",
    "    fig1 = plt.figure()\n",
    "    plt.figure(figsize=(1,1))\n",
    "    \n",
    "    img_normalized = (images[0] - images[0].min()) / (images[0].max() - images[0].min())\n",
    "    plt.imshow(img_normalized.numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    output = model.conv1(images)\n",
    "    layer_1 = output[0, :, :, :]\n",
    "    layer_1 = layer_1.data\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for idx, filt  in enumerate(layer_1):\n",
    "        if idx >= 32: continue\n",
    "        plt.subplot(4,8, idx + 1)\n",
    "        plt.imshow(filt, cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#def test_accuracy(net, dataloader):\n",
    "  ########TESTING PHASE###########\n",
    "  \n",
    "    #check accuracy on whole test set\n",
    "\n",
    "    #correct = 0\n",
    "   # total = 0\n",
    "  #  net.eval() #important for deactivating dropout and correctly use batchnorm accumulated statistics\n",
    " #   with torch.no_grad():\n",
    "        #for data in dataloader:\n",
    "        #    images, labels = data\n",
    "       #     images = images.cuda()\n",
    "      #      labels = labels.cuda()\n",
    "     #       outputs = net(images)\n",
    "    #        _, predicted = torch.max(outputs.data, 1)\n",
    "   #         total += labels.size(0)\n",
    "  #          correct += (predicted == labels).sum().item()\n",
    " #   accuracy = 100 * correct / total\n",
    "#    print('Accuracy of the network on the test set: %d %%' % (accuracy))\n",
    "#    return accuracy\n",
    "\n",
    "    \n",
    "n_classes = 2       \n",
    "      \n",
    "#function to define the convolutional network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #conv2d first parameter is the number of kernels at input (you get it from the output value of the previous layer)\n",
    "        #conv2d second parameter is the number of kernels you wanna have in your convolution, so it will be the n. of kernels at output.\n",
    "        #conv2d third, fourth and fifth parameters are, as you can read, kernel_size, stride and zero padding :)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv_final = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 4096)\n",
    "        self.fc2 = nn.Linear(4096, n_classes) #last FC for classification \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.pool(self.conv_final(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #hint: dropout goes here!\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "      ####RUNNING CODE FROM HERE:\n",
    "      \n",
    "#transform are heavily used to do simple and complex transformation and data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "     #transforms.RandomHorizontalFlip(),\n",
    "     transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     ])\n",
    "\n",
    "tree = [] #vector of nets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)\n",
    "trainset_gold=torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)\n",
    "totalClasses = len(set(trainset.targets)) #number of classes\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                           shuffle=False, num_workers=4,drop_last=True)\n",
    "#mydict = toDict(trainset)\n",
    "labelsArray = list(set(testset.targets))\n",
    "\n",
    "for j in range(0, totalClasses-2):\n",
    "\n",
    "    trainset.data, trainset.targets = datasetBalancing(toDict(trainset_gold), j)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                            shuffle=True, num_workers=4,drop_last=True)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "\n",
    "    #show images just to understand what is inside the dataset\n",
    "    #images, labels = dataiter.next()\n",
    "    #imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "    net = CNN()\n",
    "\n",
    "    ###OPTIONAL:\n",
    "    #print(\"####plotting kernels of conv1 layer:####\")\n",
    "    #plot_kernel(net)\n",
    "    ####\n",
    "\n",
    "\n",
    "    net = net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().cuda() #it already does softmax computation for use!\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001) #better convergency w.r.t simple SGD :)\n",
    "\n",
    "    ###OPTIONAL:\n",
    "    #print(\"####plotting output of conv1 layer:#####\")\n",
    "    #plot_kernel_output(net,images)  \n",
    "    ###\n",
    "\n",
    "    ########TRAINING PHASE###########\n",
    "    n_loss_print = len(trainloader)  #print every epoch, use smaller numbers if you wanna print loss more often!\n",
    "\n",
    "    n_epochs = 5\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        net.train() #important for activating dropout and correctly train batchnorm\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs and cast them into cuda wrapper\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % n_loss_print == (n_loss_print -1):    \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / n_loss_print))\n",
    "                running_loss = 0.0\n",
    "      \n",
    "  \n",
    "    \n",
    "    tree.append(net)\n",
    "test_accuracy(tree,testloader, labelsArray)\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
